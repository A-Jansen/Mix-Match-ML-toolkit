<?xml version="1.0" encoding="UTF-8"?>
<combinations xmlns:xlink="http://www.w3.org/1999/xlink">
  <images>
  <im value='labeled'>assets/photos/ld.png</im>
  <im value='unlabeled'>assets/photos/ud.png</im>
  <im value='supervised'>assets/photos/sl.png</im>
  <im value='unsupervised'>assets/photos/us.png</im>
</images>
  <combi>
    <name>Predictions based on images</name>
    <datatype>Image</datatype>
    <ability>Foresee</ability>
    <datatoken>236</datatoken>
    <abilitytoken>22</abilitytoken>
    <description>Images could also be used as input for making predictions about for example next actions, states e.g. emotional states (overlap with categorizing images, in this case will it predict your future emotion) and values (on e.g. a continuous scale). </description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Classification, regression</techterm>
    <examples>
      <ex>
        <exname>LikelyAI</exname>
        <exdescription> LikelyAI will use your images as input and predicts how popular they will be when you post it on Instagram and choose the best. In this case it is using regression as it shows a percentage score of expected popularity for each image. It uses already existing images with known popularity, and the machine learning model will try to find patterns in these.</exdescription>
        <eximage>assets/photos/likelyai.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.likelyai.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/fastai-image-regression-age-prediction-based-on-image-68294d34f2ed"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image recognition</name>
    <datatype>Image</datatype>
    <ability>Categorize</ability>
    <datatoken>236</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>Images are easy to recognize for people, but can be very hard for computers as there will always be slight variations in how the photo was taken but also in what you are capturing. You could for example want to recognize different animals of a photo or handwritten numbers and not all cats look alike and everyone's handwritten 2 looks different. ML models for this ability will learn to recognize what is shown in the images. Some models can recognize only one object in each image and others can detect multiple, as well as their location. In the case of the latter, the images will not only need a label but also a bounding box around the different objects in the image with a label for what is in each bounding box.</description>
    <capabilities>
      <c1 value="Can recognize all kind of visual input"></c1>
      <c2 value="It can learn things humans are not capable of recognizing"></c2>
    </capabilities>
    <limitations>
      <l1 value="Requires labeled data"></l1>
      <l2 value="Only recognizes on what it is trained"></l2>
      <l3></l3>
    </limitations>
    <techterm>Image classification</techterm>
    <examples>
      <ex >
        <exname>Google Lens</exname>
        <exdescription> Google Lens has many different functionalities and one of them is recognizing what is in the image. It will show you the class to which your image belongs. For example, recognize the type of flower. However, it can only recognize objects on which it has been trained, so it might not be able to recognize it.</exdescription>
        <eximage>assets/photos/googlelens.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://techengage.com/google-lens-now-detects-billion-items/">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/images/classification">Link to DIY</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image clustering</name>
    <datatype>Image</datatype>
    <ability>Cluster</ability>
    <datatoken>236</datatoken>
    <abilitytoken>124</abilitytoken>
    <description>When the images are not labeled, but you still want to find images that might belong to the same category or you might want to find similar images for other purposes. In these cases, image clustering can be used as it will try to make clusters of similar images. It might take attributes into account such as the dominant color, the shape of what is in the image or find other patterns.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Image clustering</techterm>
    <examples>
      <ex>
        <exname>Google Photos</exname>
        <exdescription>Google Photos will cluster your photos based on metadata it collected (time, location) but it will also try to cluster your images and place similar pictures together. At the end, it will also use categorization to describe what is in the cluster.</exdescription>
        <eximage>assets/photos/googlephotos.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://photos.google.com/explore"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image to text/speech</name>
    <datatype>Image</datatype>
    <ability>Communicate</ability>
    <datatoken>236</datatoken>
    <abilitytoken></abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Image recognition, speech/ text synthesis, optical character recognition</techterm>
    <examples>
      <ex>
        <exname>Alternative text</exname>
        <exdescription>Describes the content of an image placed in Word or another Microsoft appliaction</exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://blogs.microsoft.com/ai/azure-image-captioning/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/swlh/automatic-image-captioning-using-deep-learning-5e899c127387"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image recommendation</name>
    <datatype>Image</datatype>
    <ability>Recommend</ability>
    <datatoken>236</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Image recommendation can use two main approaches. The first approach is content-based filtering, in this approach the machine learning model will look at what images are similar to the one you liked in the past and recommend those new similar images to you. The second approach is collaborative filtering and in this case the machine learning model will look at what similar users liked and recommend those images(user-user) or look at which images are also often liked when image X is liked by the user and recommend those images (item-item).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Recommender system images, collaborative filtering images, content based filtering images</techterm>
    <examples>
      <ex>
        <exname>Pinterest</exname>
        <exdescription>Pinterest is build around the ability of recommending images. On Pinterest you can "pin" images and based on what you have "pinned" and on what you  have searched, it will recommend other images that you probably like as well. For this is it will use a combination of the different recommender systems, as it will show you both more images that share the same content but also images that other users liked.</exdescription>
        <eximage>assets/photos/pinterest.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://pinterest.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/geekculture/building-a-visual-similarity-based-recommendation-system-using-python-872a5bea568e"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image generation</name>
    <datatype>Image</datatype>
    <ability>Generate</ability>
    <datatoken>236</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>The ability to generate new images can be used to generate new images based on already existing images, for example when you want to augment your dataset. It can also finish your sketches based on what it thinks you are drawing or generate images based on a text prompt, which might help when generating new (design) ideas.</description>
    <capabilities>
      <c1 value="Can create new content from scratch"></c1>
      <c2 value="Can augment your dataset so you have more data to train with"></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value="The model might only create one small subset of the data and fails to generalize"></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Generative adversarial network, image generation</techterm>
    <examples>
      <ex>
        <exname>Dall-e</exname>
        <exdescription>DALL-E uses the text captions that users can input to generate new images that visualize what was written in the text. It will generate different images for the same input so it can help for example with ideating or when you want to visualize something of which you do not have a photo.</exdescription>
        <eximage>assets/photos/dall-e.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://openai.com/blog/dall-e/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text prediction</name>
    <datatype>Text</datatype>
    <ability>Foresee</ability>
    <datatoken>224</datatoken>
    <abilitytoken>22</abilitytoken>
    <description>Text prediction captures two different concepts. On the one hand you can use text as input and have for example a score associated with it. For example, predict its readability.The other concept is that you are predicting the text you are likely to type, these could be the next words or letters but also possible responses to an email. For both concepts, the text will need to be preprocessed to make it understandable for machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Autocomplete, predictive text, NLP</techterm>
    <examples>
      <ex>
        <exname>Autocomplete Google</exname>
        <exdescription>When using the Google search engine, it will predict what you are probably searching and suggest several options for completing your search query. To do this, it will use your browsing history and data from all its users to predict what the most probable search query will be.</exdescription>
        <eximage>assets/photos/autocomplete.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.google.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/build-a-simple-autocomplete-model-with-your-own-google-search-history-ead26b3b6bd4"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text recognition</name>
    <datatype>Text</datatype>
    <ability>Categorize</ability>
    <datatoken>224</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>To match text with certain predefined categories, you need to have a model that is trained on text fragments with a label. For example, movie reviews are labeled as positive or negative or emails can be labeled as spam/ no spam.To be able to recognize the texts, they need to be preprocessed to make them understandable for machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>Categories should be predefined, does not understand the text</limitations>
    <techterm>Text classification, NLP</techterm>
    <examples>
      <ex>
        <exname>Sentiment analysis</exname>
        <exdescription>This tool allows you to analyse the sentiment of a fragment of text and will predict if it is positive or negative, and also shows how confident is that it has predicted it correctly. You could use this information to monitor reviews about your product on for example social media.</exdescription>
        <eximage>assets/photos/sentimentanalysis.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://monkeylearn.com/sentiment-analysis-online/">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/text/tutorials/text_classification_rnn">Link to DIY</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text clustering</name>
    <datatype>Text</datatype>
    <ability>Cluster</ability>
    <datatoken>224</datatoken>
    <abilitytoken>124</abilitytoken>
    <description>When no labels are present, texts can be clustered based on their similarities. For example, you might want to cluster them on writing style, the language in which it is written or the topic they are covering. In order to find these similarities between texts, you will first need to preprocess the data to make it understandable for the machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Text clustering, NLP</techterm>
    <examples>
      <ex>
        <exname>Google News</exname>
        <exdescription>Google News clusters articles from different sources that cover the same topic. For this, it will look at similarities between the news articles and group them together. At the end, the extra step of categorization is done to describe what is in these clusters.</exdescription>
        <eximage>assets/photos/googlenews.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://news.google.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/making-sense-of-text-clustering-ca649c190b20"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text understanding</name>
    <datatype>Text</datatype>
    <ability>Understand</ability>
    <datatoken>242</datatoken>
    <abilitytoken></abilitytoken>
    <description>Can recognize text from e.g. sheets of papers, credit cards, receipts and convert it to digital text</description>
    <capabilities>Can recognize text in real-time</capabilities>
    <limitations>The scan/image should be of good enough quality</limitations>
    <techterm>Optical character recognition, NLP</techterm>
    <examples>
      <ex>
        <exname>OneNote</exname>
        <exdescription>Copy text from a picture into your notes</exdescription>
        <eximage>assets/photos/onenote.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://support.microsoft.com/en-us/office/copy-text-from-pictures-and-file-printouts-using-ocr-in-onenote-93a70a2f-ebcd-42dc-9f0b-19b09fd775b4">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://developers.google.com/ml-kit/vision/text-recognition">Link to diy site</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Written communication</name>
    <datatype>Text</datatype>
    <ability>Communicate</ability>
    <datatoken>224</datatoken>
    <abilitytoken></abilitytoken>
    <description>Craft messages in understandable written language</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Natural Language Processing (NLP)</techterm>
    <examples>
      <ex>
        <exname>Duolingo chatbot</exname>
        <exdescription>Helps you practice real conversations in French, German or Spanish</exdescription>
        <eximage>assets/photos/duolingo.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.tristatetechnology.com/blog/best-language-learning-chatbot-apps/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-create-a-chatbot-with-python-deep-learning-in-less-than-an-hour-56a063bdfc44"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text recommendation</name>
    <datatype>Text</datatype>
    <ability>Recommend</ability>
    <datatoken>224</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Text recommendation can be done by using techniques to get an idea of e.g. the topics that are being addressed in the text (this requires another ML ability first) and next recommend texts that have a similar topic to the one you liked or just read (content-based filtering) . Another approach is not looking at what the text is discussing, but looking at what similar users read and recommending those texts to you (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Recommender system text, text collaborative filtering, text content based filtering, NLP</techterm>
    <examples>
      <ex>
        <exname>LinkedIn</exname>
        <exdescription>LinkedIn offers job recommendations based on your job searches, job alerts, profile, and activity on LinkedIn. This data is textual data and will be used to recommend jobs that are matching with what you have searched for, what you have liked or what you have written on your profile. For this, it will also need to analyse the job description to see what is being asked.</exdescription>
        <eximage>assets/photos/linkedin.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.linkedin.com/help/linkedin/answer/11783/job-recommendations-overview?lang=en"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/@armandj.olivares/building-nlp-content-based-recommender-systems-b104a709c042"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Text generation</name>
    <datatype>Text</datatype>
    <ability>Generate</ability>
    <datatoken>224</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>Text generation can be used to generate completely new texts from scratch (based on a few key words), to complete texts or write it in a specific style (e.g. Shakespearean language). In order to have properly constructed sentences this ML ability requires another ML ability, communicate and understand, to write understandable texts.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Text generation, generative adversarial network (GAN), NLP</techterm>
    <examples>
      <ex>
        <exname>Plot generator</exname>
        <exdescription>This plot generator allows you to generate different types of texts such as fairytales, movie scripts or headlines based on a few parameters that you need to set beforehand.</exdescription>
        <eximage>assets/photos/plotgenerator.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.plot-generator.org.uk/">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/the-making-of-an-ai-storyteller-c3b8d5a983f5"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Time based predictions</name>
    <datatype>Time series</datatype>
    <ability>Foresee</ability>
    <datatoken>59</datatoken>
    <abilitytoken>22</abilitytoken>
    <description>Time series are suited for making predictions since one of the measurements (attributes) of time series data, is the time. This makes it possible to predict what could happen in the attribute by making e.g. a regression plot with the time on the X axis. Then you can either predict a continuous range of numbers or have discrete outcomes (only a certain number of categories or classes). For example, predicting the temperature tomorrow (continuous: scale of real numbers), or if it is going to rain in the upcoming hour (discrete: yes/no).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Forecasting, time series regression</techterm>
    <examples>
      <ex>
        <exname>Stock market</exname>
        <exdescription>While perfect stock market predictions are not (yet) possible, it is possible to recognize some general trends based on time series data. For the stock market, it is important to take the time component into account, which is why time series data is used. So based on the historic data, the model will try to find patterns and use these to predict the future.</exdescription>
        <eximage>assets/photos/stockmarket.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.wallstreetzen.com/stock-screener/stock-forecast"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://data-flair.training/blogs/stock-price-prediction-machine-learning-project-in-python/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Time series categorization</name>
    <datatype>Time series</datatype>
    <ability>Categorize</ability>
    <datatoken>59</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>For time series categorization (or classification) you will need to have labeled time series data. Often time series data will be first pre processed and afterwards labeled, e.g. taking the means of every second of sensor data and assign that to one category. Next the model will learn how to match the time series with the categories (or classes) and make its prediction.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Time series classification</techterm>
    <examples>
      <ex>
        <exname>Acitivity recognition</exname>
        <exdescription>To recognize activity, data from sensors such as accelerometers (measuring the acceleration in the X, Y and Z axis), ECG (heartbeat) and GPS (your location and based on this how fast you are moving) can be used. These data are time series, since one of the measurements (or features) is the time of measuring the other features. Based on the sensor data, classes can be assigned to each fragment of data and the model can learn to recognize what data pattern belongs to cycling or walking, standing still, etc.</exdescription>
        <eximage>assets/photos/activityrecognition.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://tinyurl.com/ytms74s9"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://machinelearningmastery.com/how-to-load-and-explore-a-standard-human-activity-recognition-problem/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Time series clustering</name>
    <datatype>Time series</datatype>
    <ability>Cluster</ability>
    <datatoken>59</datatoken>
    <abilitytoken>124</abilitytoken>
    <description>Time series clustering resembles time series categorization somewhat, but the difference is that with clustering, you only look at what data seems similar and group these together. So you will start with time series data, and these often need to be pre-processed. Next these data are analysed for patterns and afterwards the different data fragments will be assigned to one of the clusters.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Time series clustering</techterm>
    <examples>
      <ex>
        <exname>Clustering earthquake signals</exname>
        <exdescription>Monitoring seismic data is very labour intensive and there is more data than can be currently analysed in a supervised manner. Therefore, clustering can be used to find patterns in these data and use these clusters to for example forecast earthquakes.</exdescription>
        <eximage>assets/photos/earthquake.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.nature.com/articles/s41467-020-17841-x"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.kaggle.com/izzettunc/introduction-to-time-series-clustering"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no'>
    <name></name>
    <datatype>Time series</datatype>
    <ability>Communicate</ability>
    <datatoken>59</datatoken>
    <abilitytoken></abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm></techterm>
    <examples exist="no">
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Time series recommendation</name>
    <datatype>Time series</datatype>
    <ability>Recommend</ability>
    <datatoken>59</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Recommendations are often about what to recommend to someone based on e.g. products they bought before or music you listen often. But time series can also be used in recommendation as this will take into account when you do all these things and therefore can also take into account when to make a recommendation. In this case, an extra attribute (column) is added to your dataset with the time, while the other columns will still contain e.g. the products you bought or with which sites you interacted.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Time series recommender systems, latent dirichlet allocation time series</techterm>
    <examples>
      <ex>
        <exname>Netflix</exname>
        <exdescription>Netflix records what you  have watched, which movies you liked, etc. and also when you watched certain shows which makes it time series data. So if you are always watching the show in the morning, it might only recommend it in the morning and recommend different shows in the evening.</exdescription>
        <eximage>assets/photos/netflix.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://help.netflix.com/en/node/100639"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/create-a-recommendation-system-based-on-time-series-data-using-latent-dirichlet-allocation-2aa141b99e19"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Synthetic time series</name>
    <datatype>Time series</datatype>
    <ability>Generate</ability>
    <datatoken>59</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>Time series are data sets that has as one of the attribute the time at which the other features were measured. These can be recorded in a tabular format and/or visualized in a graph with the time on the x-axis. Based on existing data you can generate more time series data which can afterwards be used for e.g. training a better machine learning model with more data.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Generative adversarial networks (GAN), synthetic time series</techterm>
    <examples>
      <ex>
        <exname>Synthetic time series data</exname>
        <exdescription>Creating extra time series data based on some starting data. More data can help to train a better model.</exdescription>
        <eximage>assets/photos/synthetictimeseries.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.osti.gov/servlets/purl/1607585"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/creating-synthetic-time-series-data-67223ff08e34"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no'>
    <name></name>
    <datatype>Audio</datatype>
    <ability>Foresee</ability>
    <datatoken>94</datatoken>
    <abilitytoken>22</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio categorization</name>
    <datatype>Audio</datatype>
    <ability>Categorize</ability>
    <datatoken>94</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>Audio data is a special subset of time series data, and some similar techniques can be used. Using audio categorization, you can match certain segments of the audio with predefined classes (categories). For example you can recognize the pitch of sound, match the bird song to a specific bird or recognize what someone is saying. For this last example you can either have a system that does not understand what is being said but just recognizes the sounds or you can use another ML ability (understand) to interpret what is being said.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Audio classification</techterm>
    <examples>
      <ex>
        <exname>Lego Duplo Stories</exname>
        <exdescription>With Lego Duplo stories, children can experience interactive stories. For this, it is using Alexa to tell the story. During this story, Alexa will give prompts to the child to build along with their Duplo and it will ask for input for the story. For example, it will ask what the theme is of the story (limited options) and the child can say for example 'Animal story'. Machine learning is here used to recognize what is being said and incorporates that into the story.</exdescription>
        <eximage>assets/photos/legoduplostories.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.amazon.com/LEGO-System-A-S-Stories/dp/B07C225J2T"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/audio/simple_audio"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio clustering</name>
    <datatype>Audio</datatype>
    <ability>Cluster</ability>
    <datatoken>94</datatoken>
    <abilitytoken>124</abilitytoken>
    <description>Audio clustering will look for similarities between the audio fragments. Before it can do this, it is often needed to preprocess the audio by for example taking certain segments of the audio track. Next, it will find patterns in the data and cluster similar patterns together. However, it cannot tell you what is the commonality, human interpretation or labeled data is needed for this.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Audio clustering</techterm>
    <examples>
      <ex>
        <exname>Bird sounds</exname>
        <exdescription>In this project, smart segments of bird sounds were taken and used without label as the input for a machine learning model. The model created some kind of fingerprints/ or images from the sounds and next group similar sounds closer together in a two-dimensional space. This eventually led to a map with bird sounds in which similar sounding birds are grouped closer together. After this process the labels of the sounds were added again to the data so you as user can see the name of the bird.</exdescription>
        <eximage>assets/photos/birdsounds.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://experiments.withgoogle.com/ai/bird-sounds/view/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/clustering-music-to-create-your-personal-playlists-on-spotify-using-python-and-k-means-a39c4158589a"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Spoken word/speech communication
    </name>
    <datatype>Audio</datatype>
    <ability>Communicate</ability>
    <datatoken>94</datatoken>
    <abilitytoken></abilitytoken>
    <description>Create understandable messages that are being spoken</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>NLP, text to speech, speech synthesis</techterm>
    <examples>
      <ex>
        <exname>Smart assistants</exname>
        <exdescription>Smart assistents (such as Siri and Alexa) respond to your questions using understandable spoken language</exdescription>
        <eximage>assets/photos/smartassistants.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://en.wikipedia.org/wiki/Amazon_Alexa"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-build-your-own-ai-personal-assistant-using-python-f57247b4494b"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio recommendation</name>
    <datatype>Audio</datatype>
    <ability>Recommend</ability>
    <datatoken>94</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Audio recommendation can be done in two ways, it can recommend audio to the user that is similar to audio you have interacted with before/ liked. In this case it might use the metadata of e.g. music (genre, artist, etc.) or look for audio that sounds similar (see the example of audio clustering), this is content-based filtering. Another way is to recommend audio based on what similar users listened to and recommend those songs to you (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Recommender system audio, collaborative filtering audio, content based filtering audio</techterm>
    <examples>
      <ex>
        <exname>Spotify</exname>
        <exdescription>Spotify's Discover Weekly will use what you have listened to so far, as well as other information it has about you (your age, gender, location, etc.) and general information (season, hits, etc.) to recommend new music to you. For this they will use both filtering approaches as it will look at what similar users listened to and look for similar music.</exdescription>
        <eximage>assets/photos/spotify.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.spotify.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/making-your-own-discover-weekly-f1ac7546fedb"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio generation</name>
    <datatype>Audio</datatype>
    <ability>Generate</ability>
    <datatoken>94</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>Audio generation allows you to generate new audio from scratch. This could for example be new music, background noises or voices. For generating voices that speak 'human' language, you will also need another ML ability (communicate) to craft the messages that will be spoken.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Audio synthesis, generative adversarial networks (GAN)</techterm>
    <examples>
      <ex>
        <exname>AIVA</exname>
        <exdescription>AIVA is a service that creates new music for different kind of purposes. You can select a genre for the new music piece or upload a track of music to with which it should match.</exdescription>
        <eximage>assets/photos/aiva.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.aiva.ai/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/neuronio/audio-generation-with-gans-428bc2de5a89"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Predictions</name>
    <datatype>Table</datatype>
    <ability>Foresee</ability>
    <datatoken>98</datatoken>
    <abilitytoken>22</abilitytoken>
    <description> The data is stored in a table where each column holds information about one feature and each row represents one observation. Next to that, there is a column which contains the ground truth, which are the labels. These labels could be categorical (2 or more categories) or numerical (numbers on a scale). Based on all columns, except the one containing the labels, the ML model will try to predict what the label is.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Regression, classification</techterm>
    <examples>
      <ex>
        <exname>Credit card fraud detection</exname>
        <exdescription>To detect credit card fraud, all transactions are stored in a table. Each row contains the information for one transaction and each column contains the information about one feature. For this example, the features are not the original measurements but are combinations of multiple original measurements so that in the end we have less columns. Finally, one column contains the label if it was or was not a fraudulent transaction. Based on all columns, except the one with the labels, the model will try to predict if it is credit card fraud.</exdescription>
        <eximage>assets/photos/creditcardfraud.jfif</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.mastercard.us/en-us/business/issuers/business-payments/fraud-prevention.html"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Categorize tabular data</name>
    <datatype>Table</datatype>
    <ability>Categorize</ability>
    <datatoken>98</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>Each row of the table contains attributes that belong to one instance (the label which is also written down in one column of the table). When categorizing this data, the model will use all the attributes from the table (all the columns) to predict for each instance to with category it belongs.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Classification</techterm>
    <examples>
      <ex>
        <exname>Spotify</exname>
        <exdescription>Spotify for example could store the information about all the songs in a tabular format. The columns could be instruments, artist, length, etc. and one column contains the labels which in this case are the genres. The ML model will try to match each row to the correct genre based on the information in the other columns.</exdescription>
        <eximage>assets/photos/spotify.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.spotify.com/nl/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://data-flair.training/blogs/python-project-music-genre-classification/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Clustering tabular data</name>
    <datatype>Table</datatype>
    <ability>Cluster</ability>
    <datatoken>98</datatoken>
    <abilitytoken>124</abilitytoken>
    <description> Each row of the data contains information for one instance. The columns of the data are the different attributes that are measured, for example, how long you spend on one site and how many stars you gave it. Since clustering is an unsupervised learning technique, the table does not contain labels. If there are labels present, they are only used as another attribute and not to check if the prediction was correct (this would be the ability categorize).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Clustering</techterm>
    <examples>
      <ex>
        <exname>Customer clustering</exname>
        <exdescription>To cluster customers, for example to send targeted discounts, all the information of the customers is saved in a table. Each row contains the information of one customer and the columns contain the features such as the products they buy, their usual time spend in the shop and if they buy products that are discounted. Based on these attributes, the clustering model will try to find patterns among these users and group customers together who buy for example similar products. There might be one group who always buy fast food and candy, and one group who mainly buy fresh products. However, in real life these groups will be more nuanced and overlap.</exdescription>
        <eximage>assets/photos/customersegment.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.segmentify.com/blog/customer-consumer-user-segmentation-examples"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/codex/customer-segmentation-with-k-means-in-python-18336fb915be"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no'>
    <name></name>
    <datatype>Table</datatype>
    <ability>Communicate</ability>
    <datatoken>98</datatoken>
    <abilitytoken></abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Recommender system</name>
    <datatype>Table</datatype>
    <ability>Recommend</ability>
    <datatoken>98</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Recommender system will use tabular data to save all the characteristics of users and items (e.g. products). The table will have the information about one user or item stored in one row and all the attributes/ characteristics are saved in the columns. These could be the scores you give, the number of interactions you have had, if you have bought an item or saved it as favorites. Or if the rows contain the items, it could be how much it is bought and some specifics about the type of product. Based on these tables, the recommender system can either recommend for example items that are very similar to the one you liked, for this it will look in the table with items and see what other items have similar characteristics (content-based recommending). Or it will look at what other users are similar to you and recommend the items they liked (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Recommender system, collaborative filtering, content based filtering</techterm>
    <examples>
      <ex>
        <exname>Amazon</exname>
        <exdescription>Amazon uses both tables which contain the specifics of each product as well as the information about the users and their interactions with the product. So when you look at a product, it will recommend similar items based on the content of the item, for example recommend similar laptops and it will look at what other similar users bought and for example also recommend a mouse, a laptop cover or a tablet.</exdescription>
        <eximage>assets/photos/amazon.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.amazon.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.kaggle.com/saurav9786/recommender-system-using-amazon-reviews"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Synthetic tabular data</name>
    <datatype>Table</datatype>
    <ability>Generate</ability>
    <datatoken>98</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>Sometimes you will need more data than you have in order to train for example a good model or you have to take privacy issues into account and cannot share for example the patient data you have in tabular format. In both cases, you can generate new data that share the same characteristics as your original data.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Generative adversarial networks (GAN), synthetic tabular data</techterm>
    <examples>
      <ex>
        <exname>Tonic.ai</exname>
        <exdescription>TONIC.ai is a company whose speciality it is to generate new, fake, data based on the original data you provide. So you can have a dataset of tabular data and TONIC.ai will use this to generate more data that is similar but still different.</exdescription>
        <eximage>assets/photos/tonic.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.tonic.ai/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/a-step-by-step-guide-to-generate-tabular-synthetic-dataset-with-gans-d55fc373c8db"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video prediction</name>
    <datatype>Video</datatype>
    <ability>Foresee</ability>
    <datatoken>138</datatoken>
    <abilitytoken>22</abilitytoken>
    <description>Since videos can be seen as time series of images with audio, you could also use some of those techniques. For example, you could predict what the next frame of a video will be or what will be said next.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Video prediction</techterm>
    <examples>
      <ex>
        <exname>Self-driving cars</exname>
        <exdescription>Video prediction is nowadays being used for self-driving cars. Here, a camera will film the environment of the car and detect where other vehicles or people are. Using the ability foresee, they are trying to predict the next frame, which in this situation will inform you where other vehicles or people are going to. This can then be used to anticipate and redirect the trajectory of the car if needed.</exdescription>
        <eximage>assets/photos/selfdrivingcars.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.youtube.com/watch?v=yEtH23rKY8Q"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/deeppicar-part-1-102e03c83f2c"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video categorization</name>
    <datatype>Video</datatype>
    <ability>Categorize</ability>
    <datatoken>138</datatoken>
    <abilitytoken>140</abilitytoken>
    <description>Videos can be matched with categories based on their entire content. For example a video can be recognized as a sport video or a cooking video. In this case, their is one label for the entire video. Another option is to see each frame of the video as an image and have labels for each frame. In this case, machine learning models for images could also be used for recognizing categories in videos (for example recognizing cats and dogs in videos). In order to recognize multiple objects in one frame, you need, just as with images, a bounding box around each object.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Video classification</techterm>
    <examples>
      <ex>
        <exname>Zenia</exname>
        <exdescription>Zenia is an app which provide yoga classes and uses pose recognition to give personalized feedback. For this it will analyse each frame of the image, recognize the locations of the joints and next use this to see if you are doing the right pose and if you do the pose right.</exdescription>
        <eximage>assets/photos/zenia.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://zenia.app/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://thecodingtrain.com/learning/ml5/7.1-posenet.html"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video clustering</name>
    <datatype>Video</datatype>
    <ability>Cluster</ability>
    <datatoken>138</datatoken>
    <abilitytoken>124</abilitytoken>
    <description>Videos can be clustered in their entirety by looking for similar attributes of the video itself or its metadata. Next to that, it can also cluster frames within the video and this will resemble image clustering.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Video clustering</techterm>
    <examples>
      <ex>
        <exname>Video summarization</exname>
        <exdescription>In this paper, they describe a technique to use video clustering to create a summary of the video. For this they split the video in short segments, use the first frame of each segment and use these for the clustering. If the frames are not too much alike (they do not belong to the same cluster), their segment will be used for the summary.</exdescription>
        <eximage>assets/photos/videocluster.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.researchgate.net/publication/266032463_Video_Summarization_Using_Clustering"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name></name>
    <datatype>Video</datatype>
    <ability>Communicate</ability>
    <datatoken>138</datatoken>
    <abilitytoken></abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video recommendation</name>
    <datatype>Video</datatype>
    <ability>Recommend</ability>
    <datatoken>138</datatoken>
    <abilitytoken>17</abilitytoken>
    <description>Video recommendation can be based on the content and the metadata of the video. In this case, videos that share the same content and/or metadata will be recommended to you (content-based filtering). For example, when you have watched a lot of videos that explain Machine Learning it will recommend more videos on this topic. Or it will use your past behavior to find users with a similar bevhavior, e.g. you have watched or liked the same videos (collaborative-filtering). Then it will look at what these users also liked and recommend these items to you. For example, it will recommend programming videos, since similar users watched both machine learning and programming videos.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Recommender system video, collaborative filtering video, content based filtering video</techterm>
    <examples>
      <ex>
        <exname>Youtube</exname>
        <exdescription>YouTube is doing exactly what is described above, it will use both approaches to recommend new videos to you as well recommend the overall popular videos.</exdescription>
        <eximage>assets/photos/youtube.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.youtube.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.datacamp.com/community/tutorials/recommender-systems-python"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video generation</name>
    <datatype>Video</datatype>
    <ability>Generate</ability>
    <datatoken>138</datatoken>
    <abilitytoken>24</abilitytoken>
    <description>With video generation you can either create videos from scratch, based on a few parameters, or base it on already existing videos. In this last scenario, it can be very hard to distinguish real from fake, and these videos are generally known as deep fakes.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <techterm>Video generative adversarial network (GAN)</techterm>
    <examples>
      <ex>
        <exname>Deep fakes</exname>
        <exdescription>Videos that are adapted from an original video where for example faces are swapped or where the person is saying something else compared to the original video.</exdescription>
        <eximage>assets/photos/deepfake.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://edition.cnn.com/interactive/2019/01/business/pentagons-race-against-deepfakes/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://github.com/xaliceli/video-GAN"></diylink>
      </ex>
    </examples>
  </combi>
</combinations>
